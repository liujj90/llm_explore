{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF QUERY \n",
    "input: pdf\n",
    "- Save pdf as a temp txt file\n",
    "  - save each page as a separate txt file for metadata handling\n",
    "  \n",
    "- apply embeddings\n",
    "  - set number of tokens, tokens in each page, overlapping\n",
    "  \n",
    "- use a vectorstore to store embeddings\n",
    "- query using QnA\n",
    "input: question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup envrionment\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import os\n",
    "from constants import keys\n",
    "\n",
    "# llm\n",
    "from langchain import OpenAI\n",
    "\n",
    "# langchain pdf loader and vectorstore\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "# OpenAI embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# we can use a retreival question and answer api to query the document\n",
    "from langchain.chains import RetrievalQA\n",
    "# set API KEYS here\n",
    "os.environ['openai_api_key'] = keys['openai']\n",
    "\n",
    "# constants\n",
    "OPENAI_MODEL = \"text-ada-001\" # \"gpt-3.5-turbo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pypdf to load data, split into pages. This will only load text data from each page \n",
    "loader = PyPDFLoader(\"../data/ocbc_net_zero_report.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we load all the pages into a vectorstore. \n",
    "# faiss was developed by Facebook. Could explore other vectorstores, but this iwl \n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai model \n",
    "\n",
    "llm = OpenAI(model_name=OPENAI_MODEL, n=2)  \n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=faiss_index.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"give me a concise summary of this document. suggest 2 points phrased as potential questions a reader may have for this document.\"\n",
    "result = qa.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0].metadata['section'] = 'title'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting more metadata from pdf -- page summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can loop through pages to get a concise summary of each page\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "llm = OpenAI(model_name=OPENAI_MODEL, n=2)  \n",
    "qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\n",
    "content_to_summarise = pages[17].page_content\n",
    "qa_document_chain.run(input_document=content_to_summarise, question=\"give me a concise summary of this document. suggest 2 key points raised in the document.\")\n",
    "### something fishy going on with OpenAI api right now \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_to_summarise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from torch import cuda, bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"JosephusCheung/Guanaco\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"JosephusCheung/Guanaco\")\n",
    "\n",
    "device =  f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    device=device,\n",
    "    # we pass model parameters here too\n",
    "    # stopping_criteria=stopping_criteria,  # without this model will ramble\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    top_p=0.15,  # select from top tokens whose probability add up to 15%\n",
    "    top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n",
    "    max_new_tokens=64,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # penalizes repetition in tokens generated \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"It is unclear\".\n",
    "\n",
    "Context:  {context}\n",
    "\n",
    "Question: Give a concise summary of this page. Use only 1 sentence.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "\n",
    "# template for an instruction with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "llm = OpenAI(model_name=OPENAI_MODEL, n=2)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OCBC portfolio includes a large proportion of our corporate and commercial banking lending. We seek to address a large proportion of our portfolio; 42% of our corporate and commercial banking banking lending is captured within the scope of our targets. Within each sector, we have focused our targets on specific parts of the sector value chains based on the following considerations:\n",
      "• In each sector, what is the sub-sectors that are the most critical to decarbonise? Within each sector, we have identified the sub-sectors responsible for the majority of the emissions in that sector. For example, we focused on electricity generation in the Power sector and not on transmission grids, as the bulk of emissions in the Power sector arise from the generation of electricity6. By decarbonising the power generation sub-sector, a vast majority of emissions in the overall Power sector will be removed;\n",
      "• What do the sector-specific reference pathways seek to measure and address? Within a sector, reference paths are typically established for a sub-sector that is critical for sector decarbonisation. For example, we focus on emissions from aircraft burning jet fuel, and not from airport infrastructure; and\n",
      "• Which data is available for each sector? Emissions reporting is a relatively young practice, and\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.predict(context = content_to_summarise).lstrip()) # need to tweak the prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
